<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Han</title><meta name="author" content="Yinhan Zhang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 6.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Han</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://blog.csdn.net/randyhan/category_12318474.html"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/profile.png'" alt="avatar"></div><div class="author-discrip"><h3>Yinhan Zhang</h3><p class="author-bio">Data Science</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Machine Learning</h2><article><h1 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h1><h2 id="归纳偏好："><a href="#归纳偏好：" class="headerlink" title="归纳偏好："></a>归纳偏好：</h2><blockquote>
<p>判断为正例的假设有多种，我们的算法更喜欢哪一种假设将其判断为正例</p>
<p>定义：机器学习算法在学习过程中对某种类型假设的偏好，称为归纳偏好。</p>
<ul>
<li>任何机器学习算法必定有其归纳偏好，否则学得模型后，它时而告诉我们是好的，时而又是坏的，没有意义</li>
</ul>
<p>在回归问题中，有很多条曲线拟合，我们的算法必须有某种偏好，才能产生出它认为正确的模型</p>
</blockquote>
<p>奥卡姆剃刀：若有多个假设与观察一致，则选择最简单的那个</p>
<h2 id="没有免费的午餐定理："><a href="#没有免费的午餐定理：" class="headerlink" title="没有免费的午餐定理："></a>没有免费的午餐定理：</h2><blockquote>
<p>无论学习的算法A有多么聪明，算法B有多么笨拙，它们的总误差相同！！！</p>
<p>前提：所有问题出现的机会相同或所有问题同等重要</p>
</blockquote>
<p><strong>脱离具体的问题，空泛的谈什么算法更好，毫无意义</strong></p>
<h2 id="经验误差和过拟合"><a href="#经验误差和过拟合" class="headerlink" title="经验误差和过拟合"></a>经验误差和过拟合</h2><p>错误率：分类错误的样本占样本总数的比例</p>
<p>精度 &#x3D; 1- 错误率</p>
<p>学习器预测输出与实际输出之间的差异称为 “误差”</p>
<p>训练误差&#x2F;经验误差 ： 学习器在训练集上的误差</p>
<p>泛化误差：在新样本上的误差</p>
<p>我们希望在训练样本中让学习器学习到潜在样本的普遍规律，这样在遇到新样本时才能做出正确的判别</p>
<p>过拟合：</p>
<blockquote>
<p>学习器把训练样本学习的太好了，很可能把训练样本自身上的某些特点当成了所有潜在样本的特征了，这样会导致泛化性能的下降</p>
</blockquote>
<p>欠拟合：</p>
<blockquote>
<p>相反，欠拟合就是训练集上的一般性质还没有学好</p>
</blockquote>
<h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><h3 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h3><blockquote>
<p>将数据集划分出两个，一个训练集一个测试集。</p>
<p>采样方式为“分层采样”，具体过程：70%训练集，30%测试集，训练集和测试集中正例和反例的比例都相等</p>
<ul>
<li>单次使用留出法得到的结果不可靠稳定，一般采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果</li>
<li>训练集&#x2F;测试集的划分要保持数据分布的一致性，避免因数据划分过程而引入额外的偏差而对最终结果产生影响</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><blockquote>
<p>将数据集划分成k个大小相似的互斥子集，每个子集都要保持数据分布的一致性（从D中通过分层抽样得到的），k-1个子集做训练集，最后一个作为测试集，每一次划分出一个，划分十次取最后的平均值</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">clf = svm.SVC(kernel=&#x27;linear&#x27;, C=1)</span><br><span class="line">scores = cross_val_score(clf, iris.data, iris.target, cv=5)  #cv为迭代次数。</span><br><span class="line">print(scores)  # 打印输出每次迭代的度量值（准确度）</span><br><span class="line">print(&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores.mean(), scores.std() * 2))  # 获取置信区间。（也就是均值和方差）</span><br><span class="line"></span><br><span class="line"># ===================================多种度量结果======================================</span><br><span class="line">scoring = [&#x27;precision_macro&#x27;, &#x27;recall_macro&#x27;] # precision_macro为精度，recall_macro为召回率</span><br><span class="line">scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,cv=5, return_train_score=True)</span><br><span class="line">sorted(scores.keys())</span><br><span class="line">print(&#x27;测试结果：&#x27;,scores)  # scores类型为字典。包含训练得分，拟合次数， score-times （得分次数）</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分层K折交叉验证、分层随机交叉验证</span></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">3</span>)  <span class="comment">#各个类别的比例大致和完整数据集中相同</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> skf.split(iris.data, iris.target):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;分层K折划分：%s %s&quot;</span> % (train.shape, test.shape))</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">skf = StratifiedShuffleSplit(n_splits=<span class="number">3</span>)  <span class="comment"># 划分中每个类的比例和完整数据集中的相同</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> skf.split(iris.data, iris.target):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;分层随机划分：%s %s&quot;</span> % (train.shape, test.shape))</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#分组k-fold交叉验证、留一组交叉验证、留 P 组交叉验证、Group ShuffleSplit</span></span><br><span class="line"></span><br><span class="line">X = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">2.2</span>, <span class="number">2.4</span>, <span class="number">2.3</span>, <span class="number">4.55</span>, <span class="number">5.8</span>, <span class="number">8.8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">y = [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>, <span class="string">&quot;d&quot;</span>, <span class="string">&quot;d&quot;</span>]</span><br><span class="line">groups = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># k折分组</span></span><br><span class="line">gkf = GroupKFold(n_splits=<span class="number">3</span>)  <span class="comment"># 训练集和测试集属于不同的组</span></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> gkf.split(X, y, groups=groups):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;组 k-fold分割：%s %s&quot;</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留一分组</span></span><br><span class="line">logo = LeaveOneGroupOut()</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> logo.split(X, y, groups=groups):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;留一组分割：%s %s&quot;</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 留p分组</span></span><br><span class="line">lpgo = LeavePGroupsOut(n_groups=<span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> lpgo.split(X, y, groups=groups):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;留 P 组分割：%s %s&quot;</span> % (train, test))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机分组</span></span><br><span class="line">gss = GroupShuffleSplit(n_splits=<span class="number">4</span>, test_size=<span class="number">0.5</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> gss.split(X, y, groups=groups):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;随机分割：%s %s&quot;</span> % (train, test))</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="划分子集的方法"><a href="#划分子集的方法" class="headerlink" title="划分子集的方法"></a>划分子集的方法</h3><p>留一法：</p>
<blockquote>
<p>有k个样本，划分k个子集，用其中1个子集进行验证，k-1次验证，留一法在实际中的评估结果比较准确，但是计算量很大。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 留一划分子集</span><br><span class="line">loo = LeaveOneOut()</span><br><span class="line">for train, test in loo.split(iris.data):</span><br><span class="line">    print(&quot;留一划分：%s %s&quot; % (train.shape, test.shape))</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line"># 留一分组</span><br><span class="line">logo = LeaveOneGroupOut()</span><br><span class="line">for train, test in logo.split(X, y, groups=groups):</span><br><span class="line">    print(&quot;留一组分割：%s %s&quot; % (train, test))</span><br></pre></td></tr></table></figure>

<p>自助法：</p>
<blockquote>
<p>每次随机从数据集D中挑选出一个样本，将其拷贝放入D‘ ，然后再把该样本放回去（该样本下一次抽取时还能被抽取到），这个过程执行m次</p>
<ul>
<li>有一部分数据会在D’ 中重复出现</li>
<li>有一部分数据自始至终都不会抽取到 ，大约为 0.368</li>
</ul>
<p>没有被抽取到的36.8%的数据可用作测试集，这样的测试结果称为“包外估计”</p>
<ul>
<li>自助法数据集较小，难以有效划分训练集&#x2F;测试集时很有用</li>
<li>自助法能产生多个不同的数据集，这对集成学习很有帮助</li>
</ul>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># k折划分子集</span><br><span class="line">kf = KFold(n_splits=2)</span><br><span class="line">for train, test in kf.split(iris.data):</span><br><span class="line">    print(&quot;k折划分：%s %s&quot; % (train.shape, test.shape))</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line"># 留一划分子集</span><br><span class="line">loo = LeaveOneOut()</span><br><span class="line">for train, test in loo.split(iris.data):</span><br><span class="line">    print(&quot;留一划分：%s %s&quot; % (train.shape, test.shape))</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line"># 留p划分子集</span><br><span class="line">lpo = LeavePOut(p=2)</span><br><span class="line">for train, test in loo.split(iris.data):</span><br><span class="line">    print(&quot;留p划分：%s %s&quot; % (train.shape, test.shape))</span><br><span class="line">    break</span><br><span class="line"></span><br><span class="line"># 随机排列划分子集</span><br><span class="line">ss = ShuffleSplit(n_splits=3, test_size=0.25,random_state=0)</span><br><span class="line">for train_index, test_index in ss.split(iris.data):</span><br><span class="line">    print(&quot;随机排列划分：%s %s&quot; % (train.shape, test.shape))</span><br><span class="line">    break</span><br></pre></td></tr></table></figure>



<h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><blockquote>
<p>对学习器的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要衡量模型泛化能力的评价标准</p>
<p>评估学习器的性能，即预测结果与真实标记进行比较</p>
<p>回归任务最常用的就是均方误差</p>
</blockquote>
<p>查准率：</p>
<blockquote>
<p>在预测结果都是真的里面，有多少个真的是预测对的</p>
</blockquote>
<p>查全率：</p>
<blockquote>
<p>在真实结果都是真的里面，有多少个预测是真的</p>
</blockquote>
<p><img src="D:%5CHuawei%20Share%5CHuawei%20Share%5CScreenshot_20220227_155415.jpg" alt="Screenshot_20220227_155415"></p>
<p>P-R曲线：</p>
<blockquote>
<p>查准率-查全率曲线</p>
<p>尽可能的让查准率和查全率都高，找到平衡点（查准率&#x3D;查全率）</p>
</blockquote>
<p>F1度量：</p>
<p><img src="D:%5CHuawei%20Share%5CHuawei%20Share%5CScreenshot_20220227_161417.jpg" alt="Screenshot_20220227_161417"></p>
<h4 id="ROC曲线"><a href="#ROC曲线" class="headerlink" title="ROC曲线"></a>ROC曲线</h4><blockquote>
<p>真正率：所有预测结果中正例是对的，假正率：所有预测结果中反例为对的</p>
<p>真正率为纵轴，假正率为横轴</p>
<p>我们根据学习器的预测结果对样例进行排序，按此顺序逐个把样本作为正例进行预测，每次计算出两个重要的值（x,y），分别以它们为横轴、纵轴作图，得到ROC曲线</p>
</blockquote>
<p>AUC：(Area Under ROC Curve)</p>
<blockquote>
<p>比较ROC曲线下的面积，就是预测正确的</p>
<p>损失：ROC曲线上面的面积</p>
<p>AUC曲线的意义：预测是对的大于预测是错的概率，即真正例率大于假正例率的所有点的集合—&gt;面积</p>
</blockquote>
<h4 id="方差："><a href="#方差：" class="headerlink" title="方差："></a>方差：</h4><pre><code>预测值与平均值的差值的平方的期望，度量同样大小的数据集的变动导致的学习性能的变化
</code></pre>
<p>噪声：</p>
<pre><code>数据集中的值和真实的值的差的平方的期望，表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。
</code></pre>
<p>偏差：</p>
<pre><code>预测值与真实值的差值的期望，度量学习器期望预测与真实结果的偏离程度
</code></pre>
<p>泛化误差 &#x3D; 偏差+方差+噪声</p>
<h4 id="Friedman费德曼检验："><a href="#Friedman费德曼检验：" class="headerlink" title="Friedman费德曼检验："></a>Friedman费德曼检验：</h4><blockquote>
<p>交叉t检验和McNemar检验都是在一个数据集上比较两个算法的性能，更多的时候，我们是在一组数据集上检验多个算法。</p>
<p>1.在每个数据集上两两比较检验的结果</p>
<p>2.使用基于算法排序的Friedman检验</p>
</blockquote>
<p>流程：</p>
<p>使用D1-D4四个数据集对算法A\B\C进行检验，在每个数据集上根据测试性能对算法由好到坏进行排序，</p>
<p>从好到坏：1，2，3，4   性能相同赋一样的值</p>
<p>最后得出4个数据集上平均的排名，进行排序</p>
<h3 id="偏差-误差窘境"><a href="#偏差-误差窘境" class="headerlink" title="偏差-误差窘境"></a>偏差-误差窘境</h3><p>1.欠拟合状况下</p>
<blockquote>
<p>即训练不足，学习器的拟合能力不够强，训练数据的扰动不足以使学习器发生显著变化</p>
<p>此时 偏差主导了泛化错误率</p>
</blockquote>
<p>2.正常情况下</p>
<blockquote>
<p>随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学习到</p>
<p>此时 方差主导错误率</p>
</blockquote>
<p>3.过拟合情况下</p>
<blockquote>
<p>训练程度足够强，训练数据集的轻微扰动都导致学习器发生显著变化</p>
<p>此时 泛化误差的来源为 ：训练集自身的特性，非全局的特性被学习器学习到了</p>
</blockquote>
<h2 id="线性判别分析：LDA"><a href="#线性判别分析：LDA" class="headerlink" title="线性判别分析：LDA"></a>线性判别分析：LDA</h2><blockquote>
<p>思想：</p>
<p>给定训练样例集，设法将样例投影到一条直线上，使得同类的样例的投影点尽可能的近，异类样例的点尽可能的远。在对新鲜样本进行分类的时候，将其投影到同样的直线上，再根据投影点的位置来判断新的样本的类别。</p>
</blockquote>
<h2 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h2><p>核心：利用二分类的思想来解决多分类的问题</p>
<blockquote>
<p>有N个类别，我们就拆成N个2分类问题，然后为拆出的每个二分类任务训练一个分类器，在测试时，对这些分类器的预测结果进行集成以获得最终的分类结果。</p>
<p>关键：拆分与合成</p>
<p>拆分策略：</p>
<p>一对一（OVO）：</p>
<hr>
<p>将这N个类，两两配对，得到N(N-1)&#x2F;2个二分类任务，测试时，将新样本同时交给所有的分类器，于是得到N(N-1)&#x2F;2个分类结果，最终结果可通过投票产生，把预测结果最多的类别作为最终结果。</p>
<hr>
<p>一对多（OVR）：</p>
<p>每次将一个类的样例作为正例，所有的其它类的样例作为反例来训练N个分类器。</p>
<p><strong>只要有一个分类器预测的结果是正例，那么最终结果就是这个分类器正例对应的标签</strong>，其他分类器预测的不是正例就可以排除那个正例的分类器，因为负例分类器有多个，预测结果是负例，说明正确的结果位于负例中的某一类标签。</p>
<p>多对多（MVM）：</p>
<p>每次将若干个类作为正类，若干个其他类作为反类，OVM是MVM的特例。</p>
<ul>
<li>MVM的正反类构造必须有特殊的设计  —&gt; 纠错输出码ECOC</li>
</ul>
<p>编码：</p>
<p>对N个类做M次划分，每次划分将一部分类别划分为正类，一部分划分为负类，从而形成一个二分类的训练集；这样一共产生M个训练集，M个分类器。</p>
<p>解码：</p>
<p>M个分类器分别对测试样本进行预测，这些预测标记组成一个编码（一列）。将这个编码与每一个类各自的编码进行比较（正的多，还是负的多），多的就是单个分类器最终的结果</p>
<p>编码矩阵：</p>
<p>二元码：将每个类别分为正例（1）和反例（-1）</p>
<p>三元码：除了正例和反例，还有一种停用类（0）</p>
<p>所有分类对应的编码集合起来的结果（每一列投票得到一个结果（纵向），然和合并（横向）），得到测试示例的编码，</p>
<p>再与每一个类的预测结果进行对比（横向看），算出距离最小的（纵向计算）那个类别就是最终的结果</p>
</blockquote>
<p>为什么叫做纠错输出码？</p>
<p>这是因为在测试集上，ECOC有容忍和修正的能力。其中某一个分类器出错，导致测试示例的编码有错误，但是基于这个编码得出来的最后分类结果仍然是对的，这就是分类容忍和修正。</p>
<h3 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h3><blockquote>
<p>如果数据集里面不同类别的训练样例数目差别很大，会对学习过程产生困扰。</p>
<p>比如：一个数据集里面有998个反例，只有2个正例，得出来的分类器预测的结果几乎都是反例，根本没有学习到正例的特性</p>
<p>即使，样本中正例和负例差不多，但是OVO和OVM之后，可能又有类别不平衡问题了。</p>
</blockquote>
<p>从线性分类器的角度来理解：</p>
<blockquote>
<p>预测结果与阈值进行比较，比如大于0.5为正，小于0.5为负</p>
<p>y预测为正的概率 ，m+样本中正例的数目</p>
<p>正例：y&#x2F;1-y &gt; m+&#x2F;m-    </p>
<p>负例：y&#x2F;1-y &lt;m+&#x2F;m-</p>
</blockquote>
<p>解决方法：再缩放</p>
<blockquote>
<p>(y&#x2F;1-y) * (m+&#x2F;m-)  进行缩放</p>
<ul>
<li>前提是训练集是样本总体的无偏采样 ， 但是这个假设往往不成立</li>
</ul>
<ol>
<li>直接对样本中的反例进行欠采样，即去除一些反例，使得正例和反例的数目差不多</li>
<li>对正例样本进行过采样，即增加一些正例，使得正例和反例的结果相当</li>
<li>直接对原始数据集进行学习，但在用训练好的分类器进行预测，将再缩放的式子嵌入到去决策过程，称之为”阈值移动“</li>
</ol>
</blockquote>
<ul>
<li>欠采样的时间开销小于过采样，因为前者丢弃了很多反例，使得训练集的数目小于测试集</li>
<li>过采样增加了很多正例，训练集大于原始数据集</li>
<li>过采样不能简单的对初始正例样本进行重复采样，否则会产生严重的过拟合，SMOTE对训练集中的正例进行插值来产生来产生额外的正例。</li>
<li>欠采样随机丢弃反例，会导致丢弃重要的信息，EasyEnsemble算法利用集成学习机制，将反例划分为若干个集合供不同的学习器进行学习，这样对每个学习器来看，它们都欠采样了，而对全局来说就没有过采样。</li>
</ul>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h1 id="KNN算法："><a href="#KNN算法：" class="headerlink" title="KNN算法："></a>KNN算法：</h1><p>K近邻算法采用不同的特征值之间的距离方法进行分类</p>
<p>优点：精度高、对异常值不敏感、无数据假定</p>
<p>缺点：计算复杂度高、空间复杂度高</p>
<h5 id="工作原理："><a href="#工作原理：" class="headerlink" title="工作原理："></a>工作原理：</h5><p>存在一个样本集合，并且样本集合中每个标签都存在标签，即我们知道样本中每一个数据的标签与所属分类关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中特征最相似的数据（最近邻的标签）</p>
<h4 id="KNN-sklearn使用："><a href="#KNN-sklearn使用：" class="headerlink" title="KNN-sklearn使用："></a>KNN-sklearn使用：</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors</span><br><span class="line"></span><br><span class="line">nbrs = NearstNeighbors(</span><br><span class="line">    n_neighbors = <span class="number">2</span> , algorithm = <span class="string">&#x27;ball_tree/kd_tree/brute/auto&#x27;</span>).fit(X)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="KDtree、Balltree"><a href="#KDtree、Balltree" class="headerlink" title="KDtree、Balltree"></a>KDtree、Balltree</h4><p>kd-tree：二叉树结构的树，通过有效编码样本的aggerate distance（聚合信息）来减少距离计算量，不需要明确				的计算它们的距离，<strong>但是维度超过20，效率变低，维度灾难！</strong></p>
<p>ball-tree: 解决了kd-tree在高维度上的效率低下的问题，kd-tree沿笛卡尔坐标系分隔数据，ball-tree沿着超平面来				 分割数据，通过质心C和半径R定义的节点，使得每一个结点位于由R和C定义的超平面内，使用三角不等				 式减少近邻搜索点   |x+y|&lt;&#x3D;|x|+|y|  , balltre的结点时几何球形的</p>
<p>​				</p>
<table>
<thead>
<tr>
<th align="left">参数列表：</th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td align="left">X</td>
<td>(样本数，特征数即空间的维度)</td>
</tr>
<tr>
<td align="left">leaf_size</td>
<td>叶子结点的大小，默认40</td>
</tr>
<tr>
<td align="left">metric</td>
<td>距离指标表对象，用p&#x3D;2(欧几里得指标)，默认&#x3D;“minkowski”</td>
</tr>
<tr>
<td align="left">使用：</td>
<td>tree &#x3D; BallTree(X, leaf_size&#x3D;?)</td>
</tr>
<tr>
<td align="left">1</td>
<td>distance（邻近的距离）, index(邻居的下标) &#x3D; tree.query(X[ : 1] , k&#x3D;3)</td>
</tr>
<tr>
<td align="left">2</td>
<td>tree.query_radius(X[:1],r&#x3D;0.3,count_only &#x3D; True&#x2F;False是显示哪三个邻居)</td>
</tr>
</tbody></table>
<h4 id="分类："><a href="#分类：" class="headerlink" title="分类："></a>分类：</h4><table>
<thead>
<tr>
<th>RadiusNeighborsClassifier</th>
<th>KNeighborsClassifier</th>
</tr>
</thead>
<tbody><tr>
<td>基于每一个查询点的固定半径r内的邻居数量，r是指定半径float</td>
<td>高度依赖数据，通常较大的k会抑制噪声，但是使得分类界限不明显</td>
</tr>
<tr>
<td>weight关键字（权重分配）</td>
<td>uniform(统一权重)、distance（查询点与距离成反比）,强调一下距离，这种情况下距离查询点更近的点比更远的点将产生更大的影响</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>参数列表：</th>
<th>分类器参数就这几个</th>
</tr>
</thead>
<tbody><tr>
<td>fit</td>
<td>（X,y）</td>
</tr>
<tr>
<td>get_params</td>
<td></td>
</tr>
<tr>
<td>predict</td>
<td>(分类器对象)，返回预测的类标签</td>
</tr>
<tr>
<td>predict_proba</td>
<td>同，返回预测的概率估计</td>
</tr>
<tr>
<td>radius_neighbors</td>
<td>(分类器对象，r&#x3D;?)</td>
</tr>
<tr>
<td>radius_neighbors_graph</td>
<td></td>
</tr>
<tr>
<td>score</td>
<td></td>
</tr>
<tr>
<td>kneighbors</td>
<td>(X)  返回两个参数：在距离为多少的地方，是第几个样本</td>
</tr>
</tbody></table>
<h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>KNeighbors</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighbors(</span><br><span class="line">	n_neighbors = <span class="number">5</span> , 基于邻居数的回归</span><br><span class="line">	weight=<span class="string">&#x27;uniform&#x27;</span> ,</span><br><span class="line">	algorithm=<span class="string">&#x27;auto&#x27;</span> ,</span><br><span class="line">	leaf_size=<span class="number">30</span> ,</span><br><span class="line">	p=<span class="number">2</span> ,</span><br><span class="line">	metric=<span class="string">&#x27;minkowski&#x27;</span>, </span><br><span class="line">	n_jobs=<span class="literal">None</span> 默认为<span class="number">1</span></span><br><span class="line">	)</span><br></pre></td></tr></table></figure>

<p>RadiusNeighborsRegressor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.RadiusNeighborsRegressor(</span><br><span class="line">	radius=1.0 ,  基于固定半径的回归</span><br><span class="line">	weight=&#x27;uniform&#x27; ,</span><br><span class="line">	algorithm=&#x27;auto&#x27; ,</span><br><span class="line">	leaf_size=30 ,</span><br><span class="line">	p=2 ,</span><br><span class="line">	metric=&#x27;minkowski&#x27;, </span><br><span class="line">	n_jobs=None 默认为1</span><br><span class="line">	)# 决策树</span><br></pre></td></tr></table></figure>

<h4 id="分类-DecisionTreeClassifier-回归DecisionTreeRegression"><a href="#分类-DecisionTreeClassifier-回归DecisionTreeRegression" class="headerlink" title="分类 DecisionTreeClassifier \回归DecisionTreeRegression:"></a>分类 DecisionTreeClassifier \回归DecisionTreeRegression:</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sklearn.tree.DecisionTreeClassifier(</span><br><span class="line">	criteria =&#x27;gini&#x27;/&#x27;entropy&#x27;,默认基尼</span><br><span class="line">	splitter = &#123;&#x27;best&#x27;,&#x27;random&#x27;&#125;用于选择每个节点的策略</span><br><span class="line">	max_depth= </span><br><span class="line">	max_feature = &#123;&#x27;auto&#x27;,&#x27;sqrt&#x27;,&#x27;log2&#x27;&#125; , 最佳有效划分区</span><br><span class="line">	random_state =</span><br><span class="line">	class_weight = </span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">StandardScaler : 数据归一化处理</span><br><span class="line"><span class="comment">#集合管道进行操作</span></span><br><span class="line">pipe = Pipeline(step = [(<span class="string">&#x27;scaler&#x27;</span>,Standardscaler()),(<span class="string">&#x27;svc&#x27;</span>,SVC())])</span><br><span class="line"></span><br><span class="line">Pipeline的作用：</span><br><span class="line"></span><br><span class="line">    Pipeline可以将许多算法模型串联起来，可以用于把多个estamitors级联成一个estamitor,比如将特征提				取、归一化、分类组织在一起形成一个典型的机器学习问题工作流。</span><br><span class="line">    Pipleline中最后一个之外的所有estimators都必须是变换器（transformers），最后一个estimator可以是任意类型（transformer，classifier，regresser）,如果最后一个estimator是个分类器，则整个pipeline就可以作为分类器使用，如果最后一个estimator是个聚类器，则整个pipeline就可以作为聚类器使用。</span><br><span class="line"></span><br><span class="line">        主要带来两点好处：</span><br><span class="line"></span><br><span class="line">        <span class="number">1.</span>直接调用fit和predict方法来对pipeline中的所有算法模型进行训练和预测。</span><br><span class="line"></span><br><span class="line">        <span class="number">2.</span>可以结合grid search对参数进行选择.</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>串行化用法：</span><br><span class="line"></span><br><span class="line">(<span class="number">1</span>)通过steps参数，设定数据处理流程。格式为(<span class="string">&#x27;key&#x27;</span>,<span class="string">&#x27;value&#x27;</span>)，key是自己设定的名称，value是对应的处理类。最后通过<span class="built_in">list</span>将这些step传入。前n-<span class="number">1</span>个step中的类都必须有transform函数，最后一步可有可无，一般最后一步为模型。使用最简单的iris数据集来举例：</span><br><span class="line"></span><br><span class="line"><span class="keyword">in</span>[ ]:</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris=load_iris()</span><br><span class="line">pipe=Pipeline(steps=[(<span class="string">&#x27;pca&#x27;</span>,PCA()),(<span class="string">&#x27;svc&#x27;</span>,SVC())])</span><br><span class="line">pipe.fit(iris.data,iris.target)</span><br><span class="line">out[ ]:</span><br><span class="line"></span><br></pre></td></tr></table></figure></article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://blog.csdn.net/randyhan/category_12318474.html"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2023 by Yinhan Zhang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>